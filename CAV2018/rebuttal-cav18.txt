(R1) The use of normal forms.

The need for normal forms arises from the fact that the FWL effects
apply directly to the coefficients of the controller, thus the
equations supplied to the solver must be presented in a domain where
these are unaltered (i.e. Reachable and Observable Canonical Form).

(R1) Concerns about Theorem 3.

By looking at the radius of the eigenvalues we consider the envelope
of the signal, thus including overshoots.

[I don't really understand the rest]
We must also point out that the 'Hence...' being referenced here
refers to matrices with no geometric multiplicities, whereas the
example here [what example?] does not correspond to that case.  In the
case of geometric multiplicities described afterwards, we compensate
for the effect of the overshoot (by finding \underline{k}) while still
evaluating the envelope of the oscillation.

(R2) Why is the CEGIS approach different from the one in Aabate's
CAV'17 paper?

As you have correctly pointed out, one of the main contributions of
this paper is the optimization function, which, while being indeed
performed after the conterexample has been found, it does however
participate in an iterative process which contains two distinct
refinement loops [which ones? refer to figure 2 in the paper. I only
see one there happening in stage 4], so it is not simply
post-processing.

The second main contribution compared to CAV'17 is the fact that the
current submission deals with continuous time models, which CAV'17
does not. This introduces the refinement of the sampling time that
corresponds to stage 7 in Fig 2.

(R2) How much harder is to synthesise a controller+observer, compared to just a controller? What are the timings when trying to solve the current problem with the techniques in Aabate's CAV'17?

[I don't understand this -- I think you are saying that the standard
CEGIS in the paper is the one from CAV'17 but this does not follow
from the paper, CAV'17 is not standard CEGIS]

The benchmarks here are performed against the fast method used CAV17,
so the question about how it compares is answered: optimization is
much faster.  Although our results are statistical, we use the same
base benchmarks, so it is easy to infer how the introduction of the
observer(which doubles the order of the model) increases the
computation time, our fastest times using CAV17 with and observer vs
the controller case in that paper (helicopter= 57.2 vs 1.37,
inv_pendulum 277 vs .56, pendulum .8 vs .6, some are faster but median
times are much worse). We are happy to evaluate and include such
results for the full set to provide a formal comparison.

(R2) How does the optimization step compare to Raman et al. "Reactive synthesis from signal temporal logic specifications", HSCC'15?

The main difference with Raman et al. is that we are working on
different domains, namely they work with LTL models (LTL is not a
typo), while we consider PID-like controllers -- we have explained the
disadvantages of LTL models vs direct linear control in Section 1,
paragraph 2. However, given that the objective of their optimization
function is similar to ours, it might be indeed useful to extrapolate
their objective functions into their equivalent in our domain -- thank
you for the suggestion.

(R3) Why use a SAT solver and not a numerical solver given that you are discovering matrices and that you have an optimization criterion? What solver was used for the optimization step?

We use a SAT solver because we need to fit matrices to FWL effects
which are better processed using SAT.  The optimization is done using
our own algorithm described in Section 7, under "Refine abstraction
and optimize".

(R3) You mention in the Abstract step that some of the constraints will be used by the SYNTHESIZE phase to reduce the size of the solution space. How is that done? Is it just by incorporating the constraints into the solver, or is the size reduced in a more explicit way?

We incorporate the constraints in the solver by adding them to the formula.
